{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Codeblock 0 - installation of packages that are needed\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS3VIR0ipDwJ",
        "outputId": "20e89e54-4d69-40d1-f4e5-c05b1dedc982"
      },
      "id": "gS3VIR0ipDwJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.27.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3580b318",
      "metadata": {
        "id": "3580b318"
      },
      "outputs": [],
      "source": [
        "#Codeblock 1 - importing all necessary packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import random\n",
        "import re\n",
        "import io\n",
        "#Codeline 1a-importing transformers\n",
        "import transformers\n",
        "#Codeline 1b-importing logging to capture and store messgaes\n",
        "from transformers import logging\n",
        "#Codeline 1c-in the logging module of transformers, this library \n",
        "#sets the verbosity level for log messages to display errors\n",
        "logging.set_verbosity_error()\n",
        "#Codeline 1d-setting the device to run on cpu\n",
        "# DEVICE\n",
        "device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e418729",
      "metadata": {
        "id": "2e418729"
      },
      "outputs": [],
      "source": [
        "#Codeblock 2 - importing all data\n",
        "#Codeline 2a - opening the json file\n",
        "f = open('/content/gdrive/MyDrive/data.json')\n",
        "#Codeline 2b - storing json file to variable - data\n",
        "data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "825f5968",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "825f5968",
        "outputId": "21e774a6-6599-4fc5-8c26-0ea71dbf6708"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     text    label\n",
              "0  What time of day is it     date\n",
              "1         What day is it?     date\n",
              "2       What is the month     date\n",
              "3     How is Sunday going     date\n",
              "4      I have to sign off  goodbye"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5504e0a7-3603-44e4-a93e-990807d0efe6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What time of day is it</td>\n",
              "      <td>date</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What day is it?</td>\n",
              "      <td>date</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the month</td>\n",
              "      <td>date</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How is Sunday going</td>\n",
              "      <td>date</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I have to sign off</td>\n",
              "      <td>goodbye</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5504e0a7-3603-44e4-a93e-990807d0efe6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5504e0a7-3603-44e4-a93e-990807d0efe6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5504e0a7-3603-44e4-a93e-990807d0efe6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "#Codeblock 3 - importing training data\n",
        "#Codeline 3a - opening the training data created into dataframe\n",
        "# The dataset I've created\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/chat_train.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac70acbb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac70acbb",
        "outputId": "964a774b-b66e-42de-efc3-be6916dd30c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "greeting    16\n",
              "age         10\n",
              "date         7\n",
              "goodbye      7\n",
              "name         7\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "#Codeline 3b\n",
        "df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36c3fb70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36c3fb70",
        "outputId": "2c189084-d1fd-425b-9a17-b2dc962a4931"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    0.333333\n",
              "0    0.208333\n",
              "1    0.145833\n",
              "2    0.145833\n",
              "4    0.145833\n",
              "5    0.020833\n",
              "Name: label, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "#Codeblock 4 - Encoding labels\n",
        "#Codeline 4a - Encoding the labels using LabelEncoder\n",
        "# Encoding the labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['label'])\n",
        "#Codeline 4b - Checking the distribution of the labels\n",
        "#for 6 categories - 0,1,2,3,4,5\n",
        "# Checking distribution\n",
        "df['label'].value_counts(normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ce52c17",
      "metadata": {
        "id": "6ce52c17"
      },
      "outputs": [],
      "source": [
        "#Codeblock 5 - Splitting as the train text and train label\n",
        "train_text, train_labels = df['text'], df['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64ad471c",
      "metadata": {
        "id": "64ad471c"
      },
      "outputs": [],
      "source": [
        "#Codeblock 6 - Trying bert model\n",
        "# bert-base-uncased model\n",
        "\n",
        "# from transformers import AutoModel, BertTokenizerFast\n",
        "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# bert = AutoModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c25541d5",
      "metadata": {
        "id": "c25541d5"
      },
      "outputs": [],
      "source": [
        "#Codeblock 7 - Trying roberta model\n",
        "# roberta-base model\n",
        "\n",
        "# from transformers import RobertaTokenizer, RobertaModel\n",
        "# # Load the Roberta tokenizer\n",
        "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "# # Import Roberta pretrained model\n",
        "# bert = RobertaModel.from_pretrained('roberta-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "152e9d37",
      "metadata": {
        "id": "152e9d37"
      },
      "outputs": [],
      "source": [
        "#Codeblock 8 - Trying distilbert model\n",
        "# distilbert-base-uncased model\n",
        "#Codeline 8a - importing necessary packages\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "#Codeline 8b - loading tokenizer from pretrained model to variable tokenizer\n",
        "# Load tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "#Codeline 8c - importing pretrained model to variable bert\n",
        "# Import pretrained model\n",
        "bert = DistilBertModel.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0be8890",
      "metadata": {
        "id": "c0be8890",
        "outputId": "27d5e6bb-abd8-4253-abfe-98da0797d928"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[  101,  2023,  2003,  1037, 14324,  2944,  1012,   102],\n",
            "        [  101,  2070,  3231,  6251,   102,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 0, 0, 0]])}\n"
          ]
        }
      ],
      "source": [
        "#Codeblock 9 - Encoding\n",
        "#Codeline 9a - Define some text\n",
        "text = [\"this is a bert model.\", \"some test sentence\"]\n",
        "#Codeline 9b - Encode the text using tokenizer and print it\n",
        "# Encode the text\n",
        "encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "print(encoded_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc612216",
      "metadata": {
        "id": "cc612216",
        "outputId": "ac69a3d2-1f08-41c9-ca01-50c7c0fd0703"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Codeblock 10 - Define sequence length\n",
        "#Codeline 10a - Based on length of train_text words, find sequence length\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "#Codeline 10b - Create distribution of sequence length\n",
        "pd.Series(seq_len).hist(bins = 9)\n",
        "#Codeline 10c - Select the maximum sequence length as 9\n",
        "# Based on the histogram we select max sequence length as 9\n",
        "max_seq_len = 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13e3cdcd",
      "metadata": {
        "id": "13e3cdcd"
      },
      "outputs": [],
      "source": [
        "#Codeblock 11 - Tokenize the sequences in train data\n",
        "# Tokenization of the sequences in train data\n",
        "\n",
        "#Codeline 11a - define tokenizer\n",
        "tokens_train = tokenizer(\n",
        "#Codeline 11b - make the train_text to a list\n",
        "    train_text.tolist(),\n",
        "#Codeline 11c - define maximum length as sequence length\n",
        "    max_length = max_seq_len,\n",
        "#Codeline 11d - define the padding\n",
        "    padding='longest',\n",
        "#Codeline 11e - define truncation\n",
        "    truncation=True,\n",
        "#Codeline 11f - return the token_type_ids\n",
        "    return_token_type_ids=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9540b36",
      "metadata": {
        "id": "a9540b36"
      },
      "outputs": [],
      "source": [
        "#Codeblock 12 - Define train sequence, attention mask and target as tensors\n",
        "#Codeline 12a- Define train sequence tensor\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "#Codeline 12b- Define attention mask tensor\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "#Codeline 12c- Define target tensor\n",
        "train_y = torch.tensor(train_labels.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "838e644e",
      "metadata": {
        "id": "838e644e"
      },
      "outputs": [],
      "source": [
        "#Codeblock 13 - Create tensor dataset\n",
        "#Codeline 13a - importing necessary packages \n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "#Codeline 13b - defining batch size\n",
        "batch_size = 16\n",
        "#Codeline 13c - defining train_data as tensor data set\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "#Codeline 13d - defining train_sampler as RandomSampler\n",
        "train_sampler = RandomSampler(train_data)\n",
        "#Codeline 13e - defining dataloader uing train tensor data set. \n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cba9f20c",
      "metadata": {
        "id": "cba9f20c"
      },
      "outputs": [],
      "source": [
        "#Codeblock 14 - define class BERT\n",
        "class BERT(nn.Module):\n",
        "    #Codeline 14a - define bert\n",
        "    def __init__(self, bert):      \n",
        "        super(BERT, self).__init__()\n",
        "        self.bert = bert \n",
        "        #Codeline 14b - add dropout \n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        #Codeline 14c - add ReLU \n",
        "        # Activation\n",
        "        self.relu =  nn.ReLU()\n",
        "        #Codeline 14d - dense layers\n",
        "        # Dense layer\n",
        "        self.fc1 = nn.Linear(768,512)\n",
        "        self.fc2 = nn.Linear(512,256)\n",
        "        self.fc3 = nn.Linear(256,6)\n",
        "        #Codeline 14e - softmax activation\n",
        "        # Softmax activation\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "    #Codeline 14f -  Forward pass for torch   \n",
        "    # Forward pass for Torch\n",
        "    def forward(self, sent_id, mask):\n",
        "        #Codeline 14g - fully connected bert\n",
        "        # Fully connected\n",
        "        x = self.bert(sent_id, attention_mask=mask)[0][:,0]\n",
        "        #Codeline 14h - fc1\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        #Codeline 14i - fc2\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        #Codeline 14j - output layer\n",
        "        # Output layer\n",
        "        x = self.fc3(x)\n",
        "        #Codeline 14k - softmax\n",
        "        # Apply softmax\n",
        "        x = self.softmax(x)\n",
        "        #Codeline 14l - return x\n",
        "        return x\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aab25885",
      "metadata": {
        "id": "aab25885",
        "outputId": "702ddb95-d271-46a3-d1c4-b64e065f0755"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "================================================================================\n",
              "Layer (type:depth-idx)                                  Param #\n",
              "================================================================================\n",
              "BERT                                                    --\n",
              "├─DistilBertModel: 1-1                                  --\n",
              "│    └─Embeddings: 2-1                                  --\n",
              "│    │    └─Embedding: 3-1                              (23,440,896)\n",
              "│    │    └─Embedding: 3-2                              (393,216)\n",
              "│    │    └─LayerNorm: 3-3                              (1,536)\n",
              "│    │    └─Dropout: 3-4                                --\n",
              "│    └─Transformer: 2-2                                 --\n",
              "│    │    └─ModuleList: 3-5                             (42,527,232)\n",
              "├─Dropout: 1-2                                          --\n",
              "├─ReLU: 1-3                                             --\n",
              "├─Linear: 1-4                                           393,728\n",
              "├─Linear: 1-5                                           131,328\n",
              "├─Linear: 1-6                                           1,542\n",
              "├─LogSoftmax: 1-7                                       --\n",
              "================================================================================\n",
              "Total params: 66,889,478\n",
              "Trainable params: 526,598\n",
              "Non-trainable params: 66,362,880\n",
              "================================================================================"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Codeblock 15 - prevent model weights update for fine tuning\n",
        "# Prevent model weights update for fine-tuning.\n",
        "for param in bert.parameters():\n",
        "      #Codeline 15a - requires_grad=False\n",
        "      param.requires_grad = False\n",
        "#Codeline 15b - BERT model        \n",
        "model = BERT(bert)\n",
        "\n",
        "#Codeline 15c - Load the bert model into device\n",
        "# Load the model into device \n",
        "model = model.to(device)\n",
        "#Codeline 15d - summary of model\n",
        "from torchinfo import summary\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98cfb885",
      "metadata": {
        "id": "98cfb885"
      },
      "outputs": [],
      "source": [
        "#Codeblock 16 - optimizer\n",
        "# AdamW optimizer\n",
        "\n",
        "\n",
        "from transformers import AdamW\n",
        "from torch.optim import AdamW\n",
        "#Codeline 16a - AdamW optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a24f2235",
      "metadata": {
        "id": "a24f2235",
        "outputId": "cd7176de-f2eb-4c3d-f994-9a62c1f7a557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.8        1.14285714 1.14285714 0.5        1.14285714 8.        ]\n"
          ]
        }
      ],
      "source": [
        "#Codeblock 17 - computing the class weights\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "# Computation of the class weights\n",
        "#Codeline 17a - compute balanced class weights and print\n",
        "class_weights = compute_class_weight( class_weight='balanced',\n",
        "                                  classes=np.unique(train_labels),\n",
        "                                  y=train_labels )\n",
        "print(class_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9618c704",
      "metadata": {
        "id": "9618c704"
      },
      "outputs": [],
      "source": [
        "#Codeblock 18 - Make the class weights to tensor\n",
        "#Codeline 18a - class weights to tensor, to device\n",
        "# Class weights to tensor\n",
        "weights= torch.tensor(class_weights, dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "#Codeline 18b - define negative log-likelihood as loss function\n",
        "# Loss function: Negative log-likelihood\n",
        "cross_entropy = nn.NLLLoss(weight=weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c595863e",
      "metadata": {
        "id": "c595863e"
      },
      "outputs": [],
      "source": [
        "#Codeblock 19 - Define training loss, number of epochs and learning rate\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "#Codeline 19a - Define train_losses list\n",
        "train_losses=[]\n",
        "#Codeline 19b - Define number of epochs\n",
        "# n epochs\n",
        "n_epochs = 200\n",
        "#Codeline 19c - Define learning rate with optimizer, step size and gamma value\n",
        "# learning rate scheduler to improve results\n",
        "step_lr_sscheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e319a58b",
      "metadata": {
        "id": "e319a58b"
      },
      "outputs": [],
      "source": [
        "#Codeblock 20 - Model training\n",
        "# Model training\n",
        "def train():\n",
        "    #Codeline 20a - model train\n",
        "    model.train()\n",
        "    #Codeline 20b - define total loss\n",
        "    total_loss = 0\n",
        "    #Codeline 20c - define total predictions\n",
        "    total_predictions=[]\n",
        "    #Codeline 20d - iterate through train_dataloader\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        #Codeline 20e - loading the batch into device\n",
        "        # Load the batch into device\n",
        "        batch = [r.to(device) for r in batch] \n",
        "        sent_id, mask, labels = batch\n",
        "        #Codeline 20f - Get model predictions for current batch\n",
        "        # Get model predictions for the current batch\n",
        "        predictions = model(sent_id, mask)\n",
        "        #Codeline 20g - find cross-entropy loss\n",
        "        # Loss between actual and predicted labels\n",
        "        loss = cross_entropy(predictions, labels)\n",
        "        #Codeline 20h - Find total loss\n",
        "        # Add on to the total loss\n",
        "        total_loss =+ loss.item()\n",
        "        #Codeline 20i - Backward propagation for gradient calculation\n",
        "        # Backward propagation for gradient calculation\n",
        "        loss.backward()\n",
        "        #Codeline 20j - To prevent from exploding gradients, we set the upper bounding to 1\n",
        "        # Upper bounding the the gradients to 1.0 to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        \n",
        "        optimizer.step()\n",
        "        #Codeline 20k - Reset gradients\n",
        "        # Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "        #Codeline 20l - Prediction using GPU and CPU\n",
        "        # Predictions usually stored in the GPU (however I use CPU as well). We load onto CPU anyway.\n",
        "        predictions=predictions.detach().cpu().numpy()\n",
        "        #Codeline 20m - Predictions to batch \n",
        "        # Save predictions of the batch\n",
        "        total_predictions.append(predictions)\n",
        "    #Codeline 20n - Calculate average training loss of epoch     \n",
        "    # Training loss of the epoch\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    #Codeline 20o -  Concatenate total predictions\n",
        "    # Predictions -> (n batches, batch_size, n classes)\n",
        "    # Reshape predictions into -> (n samples, n classes)\n",
        "    total_predictions  = np.concatenate(total_predictions, axis=0)\n",
        "    #Codeline 20p - Return print the average loss and predictions\n",
        "    # Return average loss and predictions\n",
        "    return avg_loss, total_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dee3b97",
      "metadata": {
        "scrolled": true,
        "id": "5dee3b97",
        "outputId": "2d7708cf-c3d3-463a-e706-903ac625bb2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 200 \n",
            "\n",
            " Epoch 2 / 200 \n",
            "\n",
            " Epoch 3 / 200 \n",
            "\n",
            " Epoch 4 / 200 \n",
            "\n",
            " Epoch 5 / 200 \n",
            "\n",
            " Epoch 6 / 200 \n",
            "\n",
            " Epoch 7 / 200 \n",
            "\n",
            " Epoch 8 / 200 \n",
            "\n",
            " Epoch 9 / 200 \n",
            "\n",
            " Epoch 10 / 200 \n",
            "\n",
            " Epoch 11 / 200 \n",
            "\n",
            " Epoch 12 / 200 \n",
            "\n",
            " Epoch 13 / 200 \n",
            "\n",
            " Epoch 14 / 200 \n",
            "\n",
            " Epoch 15 / 200 \n",
            "\n",
            " Epoch 16 / 200 \n",
            "\n",
            " Epoch 17 / 200 \n",
            "\n",
            " Epoch 18 / 200 \n",
            "\n",
            " Epoch 19 / 200 \n",
            "\n",
            " Epoch 20 / 200 \n",
            "\n",
            " Epoch 21 / 200 \n",
            "\n",
            " Epoch 22 / 200 \n",
            "\n",
            " Epoch 23 / 200 \n",
            "\n",
            " Epoch 24 / 200 \n",
            "\n",
            " Epoch 25 / 200 \n",
            "\n",
            " Epoch 26 / 200 \n",
            "\n",
            " Epoch 27 / 200 \n",
            "\n",
            " Epoch 28 / 200 \n",
            "\n",
            " Epoch 29 / 200 \n",
            "\n",
            " Epoch 30 / 200 \n",
            "\n",
            " Epoch 31 / 200 \n",
            "\n",
            " Epoch 32 / 200 \n",
            "\n",
            " Epoch 33 / 200 \n",
            "\n",
            " Epoch 34 / 200 \n",
            "\n",
            " Epoch 35 / 200 \n",
            "\n",
            " Epoch 36 / 200 \n",
            "\n",
            " Epoch 37 / 200 \n",
            "\n",
            " Epoch 38 / 200 \n",
            "\n",
            " Epoch 39 / 200 \n",
            "\n",
            " Epoch 40 / 200 \n",
            "\n",
            " Epoch 41 / 200 \n",
            "\n",
            " Epoch 42 / 200 \n",
            "\n",
            " Epoch 43 / 200 \n",
            "\n",
            " Epoch 44 / 200 \n",
            "\n",
            " Epoch 45 / 200 \n",
            "\n",
            " Epoch 46 / 200 \n",
            "\n",
            " Epoch 47 / 200 \n",
            "\n",
            " Epoch 48 / 200 \n",
            "\n",
            " Epoch 49 / 200 \n",
            "\n",
            " Epoch 50 / 200 \n",
            "\n",
            " Epoch 51 / 200 \n",
            "\n",
            " Epoch 52 / 200 \n",
            "\n",
            " Epoch 53 / 200 \n",
            "\n",
            " Epoch 54 / 200 \n",
            "\n",
            " Epoch 55 / 200 \n",
            "\n",
            " Epoch 56 / 200 \n",
            "\n",
            " Epoch 57 / 200 \n",
            "\n",
            " Epoch 58 / 200 \n",
            "\n",
            " Epoch 59 / 200 \n",
            "\n",
            " Epoch 60 / 200 \n",
            "\n",
            " Epoch 61 / 200 \n",
            "\n",
            " Epoch 62 / 200 \n",
            "\n",
            " Epoch 63 / 200 \n",
            "\n",
            " Epoch 64 / 200 \n",
            "\n",
            " Epoch 65 / 200 \n",
            "\n",
            " Epoch 66 / 200 \n",
            "\n",
            " Epoch 67 / 200 \n",
            "\n",
            " Epoch 68 / 200 \n",
            "\n",
            " Epoch 69 / 200 \n",
            "\n",
            " Epoch 70 / 200 \n",
            "\n",
            " Epoch 71 / 200 \n",
            "\n",
            " Epoch 72 / 200 \n",
            "\n",
            " Epoch 73 / 200 \n",
            "\n",
            " Epoch 74 / 200 \n",
            "\n",
            " Epoch 75 / 200 \n",
            "\n",
            " Epoch 76 / 200 \n",
            "\n",
            " Epoch 77 / 200 \n",
            "\n",
            " Epoch 78 / 200 \n",
            "\n",
            " Epoch 79 / 200 \n",
            "\n",
            " Epoch 80 / 200 \n",
            "\n",
            " Epoch 81 / 200 \n",
            "\n",
            " Epoch 82 / 200 \n",
            "\n",
            " Epoch 83 / 200 \n",
            "\n",
            " Epoch 84 / 200 \n",
            "\n",
            " Epoch 85 / 200 \n",
            "\n",
            " Epoch 86 / 200 \n",
            "\n",
            " Epoch 87 / 200 \n",
            "\n",
            " Epoch 88 / 200 \n",
            "\n",
            " Epoch 89 / 200 \n",
            "\n",
            " Epoch 90 / 200 \n",
            "\n",
            " Epoch 91 / 200 \n",
            "\n",
            " Epoch 92 / 200 \n",
            "\n",
            " Epoch 93 / 200 \n",
            "\n",
            " Epoch 94 / 200 \n",
            "\n",
            " Epoch 95 / 200 \n",
            "\n",
            " Epoch 96 / 200 \n",
            "\n",
            " Epoch 97 / 200 \n",
            "\n",
            " Epoch 98 / 200 \n",
            "\n",
            " Epoch 99 / 200 \n",
            "\n",
            " Epoch 100 / 200 \n",
            "\n",
            " Epoch 101 / 200 \n",
            "\n",
            " Epoch 102 / 200 \n",
            "\n",
            " Epoch 103 / 200 \n",
            "\n",
            " Epoch 104 / 200 \n",
            "\n",
            " Epoch 105 / 200 \n",
            "\n",
            " Epoch 106 / 200 \n",
            "\n",
            " Epoch 107 / 200 \n",
            "\n",
            " Epoch 108 / 200 \n",
            "\n",
            " Epoch 109 / 200 \n",
            "\n",
            " Epoch 110 / 200 \n",
            "\n",
            " Epoch 111 / 200 \n",
            "\n",
            " Epoch 112 / 200 \n",
            "\n",
            " Epoch 113 / 200 \n",
            "\n",
            " Epoch 114 / 200 \n",
            "\n",
            " Epoch 115 / 200 \n",
            "\n",
            " Epoch 116 / 200 \n",
            "\n",
            " Epoch 117 / 200 \n",
            "\n",
            " Epoch 118 / 200 \n",
            "\n",
            " Epoch 119 / 200 \n",
            "\n",
            " Epoch 120 / 200 \n",
            "\n",
            " Epoch 121 / 200 \n",
            "\n",
            " Epoch 122 / 200 \n",
            "\n",
            " Epoch 123 / 200 \n",
            "\n",
            " Epoch 124 / 200 \n",
            "\n",
            " Epoch 125 / 200 \n",
            "\n",
            " Epoch 126 / 200 \n",
            "\n",
            " Epoch 127 / 200 \n",
            "\n",
            " Epoch 128 / 200 \n",
            "\n",
            " Epoch 129 / 200 \n",
            "\n",
            " Epoch 130 / 200 \n",
            "\n",
            " Epoch 131 / 200 \n",
            "\n",
            " Epoch 132 / 200 \n",
            "\n",
            " Epoch 133 / 200 \n",
            "\n",
            " Epoch 134 / 200 \n",
            "\n",
            " Epoch 135 / 200 \n",
            "\n",
            " Epoch 136 / 200 \n",
            "\n",
            " Epoch 137 / 200 \n",
            "\n",
            " Epoch 138 / 200 \n",
            "\n",
            " Epoch 139 / 200 \n",
            "\n",
            " Epoch 140 / 200 \n",
            "\n",
            " Epoch 141 / 200 \n",
            "\n",
            " Epoch 142 / 200 \n",
            "\n",
            " Epoch 143 / 200 \n",
            "\n",
            " Epoch 144 / 200 \n",
            "\n",
            " Epoch 145 / 200 \n",
            "\n",
            " Epoch 146 / 200 \n",
            "\n",
            " Epoch 147 / 200 \n",
            "\n",
            " Epoch 148 / 200 \n",
            "\n",
            " Epoch 149 / 200 \n",
            "\n",
            " Epoch 150 / 200 \n",
            "\n",
            " Epoch 151 / 200 \n",
            "\n",
            " Epoch 152 / 200 \n",
            "\n",
            " Epoch 153 / 200 \n",
            "\n",
            " Epoch 154 / 200 \n",
            "\n",
            " Epoch 155 / 200 \n",
            "\n",
            " Epoch 156 / 200 \n",
            "\n",
            " Epoch 157 / 200 \n",
            "\n",
            " Epoch 158 / 200 \n",
            "\n",
            " Epoch 159 / 200 \n",
            "\n",
            " Epoch 160 / 200 \n",
            "\n",
            " Epoch 161 / 200 \n",
            "\n",
            " Epoch 162 / 200 \n",
            "\n",
            " Epoch 163 / 200 \n",
            "\n",
            " Epoch 164 / 200 \n",
            "\n",
            " Epoch 165 / 200 \n",
            "\n",
            " Epoch 166 / 200 \n",
            "\n",
            " Epoch 167 / 200 \n",
            "\n",
            " Epoch 168 / 200 \n",
            "\n",
            " Epoch 169 / 200 \n",
            "\n",
            " Epoch 170 / 200 \n",
            "\n",
            " Epoch 171 / 200 \n",
            "\n",
            " Epoch 172 / 200 \n",
            "\n",
            " Epoch 173 / 200 \n",
            "\n",
            " Epoch 174 / 200 \n",
            "\n",
            " Epoch 175 / 200 \n",
            "\n",
            " Epoch 176 / 200 \n",
            "\n",
            " Epoch 177 / 200 \n",
            "\n",
            " Epoch 178 / 200 \n",
            "\n",
            " Epoch 179 / 200 \n",
            "\n",
            " Epoch 180 / 200 \n",
            "\n",
            " Epoch 181 / 200 \n",
            "\n",
            " Epoch 182 / 200 \n",
            "\n",
            " Epoch 183 / 200 \n",
            "\n",
            " Epoch 184 / 200 \n",
            "\n",
            " Epoch 185 / 200 \n",
            "\n",
            " Epoch 186 / 200 \n",
            "\n",
            " Epoch 187 / 200 \n",
            "\n",
            " Epoch 188 / 200 \n",
            "\n",
            " Epoch 189 / 200 \n",
            "\n",
            " Epoch 190 / 200 \n",
            "\n",
            " Epoch 191 / 200 \n",
            "\n",
            " Epoch 192 / 200 \n",
            "\n",
            " Epoch 193 / 200 \n",
            "\n",
            " Epoch 194 / 200 \n",
            "\n",
            " Epoch 195 / 200 \n",
            "\n",
            " Epoch 196 / 200 \n",
            "\n",
            " Epoch 197 / 200 \n",
            "\n",
            " Epoch 198 / 200 \n",
            "\n",
            " Epoch 199 / 200 \n",
            "\n",
            " Epoch 200 / 200 \n",
            "\n",
            "Loss: 0.000\n"
          ]
        }
      ],
      "source": [
        "#Codeblock 21 - Training over epochs\n",
        "#Codeline 21a - iterate over epochs\n",
        "for epoch in range(n_epochs):\n",
        "     #Codeline 21b - print the epoch\n",
        "    print('\\n Epoch {:} / {:} \\r'.format(epoch + 1, n_epochs))\n",
        "    #Codeline 21c - Train the model\n",
        "    # Train model\n",
        "    train_loss, _ = train()\n",
        "    #Codeline 21d - Save the training and validation loss\n",
        "    # Save training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    #Codeline 21e - deterministic reproducability\n",
        "    # Deterministicity for reproducibility of the train process for further needs\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "print(f'\\nLoss: {train_loss:.3f}\\r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8b2f0a2",
      "metadata": {
        "id": "f8b2f0a2"
      },
      "outputs": [],
      "source": [
        "#Codeblock 22 - get prediction function\n",
        "\n",
        "def get_prediction(strin):\n",
        "    #Codeline 22a - use regex\n",
        "    strin = re.sub(r'[^a-zA-Z ]+', '', strin)\n",
        "    test_text = [strin]\n",
        "    model.eval()\n",
        "    #Codeline 22b - Tokenizer\n",
        "    tokens_test_data = tokenizer(\n",
        "    test_text,\n",
        "    max_length = max_seq_len,\n",
        "    padding='longest',\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        "    )\n",
        "    #Codeline 22c - To tensor inpute ids\n",
        "    test_seq = torch.tensor(tokens_test_data['input_ids'])\n",
        "    #Codeline 22d - To tensor attention mask\n",
        "    test_mask = torch.tensor(tokens_test_data['attention_mask'])\n",
        "    \n",
        "    predictions = None\n",
        "    #Codeline 22e - predictions\n",
        "    with torch.no_grad():\n",
        "      predictions = model(test_seq.to(device), test_mask.to(device))\n",
        "    #Codeline 22f - predictions detach from cpu\n",
        "    predictions = predictions.detach().cpu().numpy()\n",
        "    #Codeline 22g - use the argmax function\n",
        "    predictions = np.argmax(predictions, axis = 1)\n",
        "    print('Intent:', le.inverse_transform(predictions)[0])\n",
        "    #Codeline 22h - decode the labels and return\n",
        "    # Return decoded labels\n",
        "    return le.inverse_transform(predictions)[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Codeblock 23 - bot response function\n",
        "def bot_response(message):\n",
        "  #Codeline 23a - define result \n",
        "  result = ''\n",
        "  #Codeline 23b - define intent using get_prediction function\n",
        "  intent = get_prediction(message)\n",
        "  #Codeline 23c - search for tag matching intent   \n",
        "  for i in data['intents']:\n",
        "    if i['tag'] == intent:\n",
        "        #Codeline 23d - from the pool of responses choose random answer\n",
        "        result = random.choice(i['responses'])\n",
        "        break\n",
        "\n",
        "  print(f\"Response : {result}\")\n",
        "  #return result"
      ],
      "metadata": {
        "id": "-xHaL-HoTQpS"
      },
      "id": "-xHaL-HoTQpS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4055c319",
      "metadata": {
        "id": "4055c319",
        "outputId": "8c95858b-4431-4f23-bdc6-b3b5a20e4795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intent: greeting\n",
            "Response : Hello\n"
          ]
        }
      ],
      "source": [
        "#Codeblock 24 - test1 a bot_response\n",
        "bot_response('Hello bot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e94eab40",
      "metadata": {
        "id": "e94eab40",
        "outputId": "074ccd9a-66ae-48f1-cf8c-09c2f93f764f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intent: greeting\n",
            "Response : How are you doing?\n"
          ]
        }
      ],
      "source": [
        "#Codeblock 25 - test2 a bot_response\n",
        "bot_response('Great, how are you?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7596fbf0",
      "metadata": {
        "id": "7596fbf0",
        "outputId": "0931f175-2497-4116-f3f0-de27be187f24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intent: name\n",
            "Response : BOTTT\n"
          ]
        }
      ],
      "source": [
        "#Codeblock 26 - test3 a bot_response\n",
        "bot_response('what is your name')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71171e07",
      "metadata": {
        "id": "71171e07",
        "outputId": "f90e591d-a2f9-4949-ac19-f727fe1d3a89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intent: date\n",
            "Response : I am available all day\n"
          ]
        }
      ],
      "source": [
        "#Codeblock 27 - test4 a bot_response\n",
        "bot_response('What time is it?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03bae777",
      "metadata": {
        "id": "03bae777",
        "outputId": "4e1c3451-5fe8-4968-b549-0ad3877869d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intent: goodbye\n",
            "Response : Best regards.\n"
          ]
        }
      ],
      "source": [
        "#Codeblock 28 - test5 a bot_response\n",
        "bot_response('Farewell!!')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}